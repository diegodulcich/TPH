{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcNe_-frZZ10"
   },
   "source": [
    "# TP4 - TEXT MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvIIw8kPtOqq"
   },
   "source": [
    "## Visión General del Notebook\n",
    "\n",
    "En esta notebook vamos a ver un ejemplo básico de **text minning aplicado a cuentos de Hernán Casciari** que están disponibles en internet. El objetivo de text mining es extraer informacion util de una base de datos de texto (corpus). El corpus que veremos aqui esta compuesto por los cuentos que Casciari fue subiendo a su blog, en forma de posteos, a lo largo de los años.\n",
    "\n",
    "El ejercicio integra conceptos fundamentales de NLP como:\n",
    "\n",
    "* Preprocesamiento de texto\n",
    "* Tokenización\n",
    "* Eliminación de stop words\n",
    "* Vectorización de texto\n",
    "* Análisis de frecuencias\n",
    "* Visualización de datos textuales\n",
    "\n",
    "Las librerías fundamentales utilizadas son:\n",
    "\n",
    "* pandas: Para manipulación y análisis de datos estructurados\n",
    "* re: Para procesamiento de expresiones regulares\n",
    "* nltk: Natural Language Toolkit, fundamental en NLP\n",
    "* sklearn: Para vectorización de texto y otras herramientas de ML\n",
    "* wordcloud: Para visualización de frecuencias de palabras\n",
    "* matplotlib: Para visualizaciones generales\n",
    "\n",
    "\n",
    "Para este ejercicio integrador vamos a basarnos en este [tutorial](https://www.aprendemachinelearning.com/ejercicio-nlp-cuentos-de-hernan-casciari-python-espanol/) donde la primera parte se trataba de **scrapear** el blog para obtener los textos. Sin embargo la url del blog ya no está disponible y los cuentos se pueden adquirir [aquí](https://hernancasciari.com/libros/) junto a otros contenidos super interesantes como sus audiolibros (sí, es un excelente narrador también).\n",
    "\n",
    "La idea entonces es tratar de ver \"**que tenia casciari en su cabeza**\" cuando escribio estos cuentos desde 2004 hasta 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRIB2XaduKHq"
   },
   "source": [
    "# Introducción al Text Mining con Cuentos de Hernán Casciari\n",
    "\n",
    "## Text Mining\n",
    "\n",
    "El text mining o minería de texto es una técnica que permite extraer información valiosa y patrones significativos de grandes volúmenes de texto no estructurado. En este notebook, realizaremos un análisis de text mining sobre los cuentos del escritor argentino Hernán Casciari, publicados en su blog entre 2004 y 2015.\n",
    "\n",
    "## Objetivos del Análisis\n",
    "- Explorar los patrones temáticos en la escritura de Casciari\n",
    "- Analizar la evolución de su vocabulario a lo largo del tiempo\n",
    "- Identificar temas recurrentes en sus cuentos\n",
    "- Visualizar las palabras más significativas por año\n",
    "\n",
    "## Datos\n",
    "Los textos analizados provienen de los cuentos publicados en el blog del autor. Aunque originalmente estos textos fueron scrapeados de su blog, actualmente se pueden encontrar en su sitio web oficial (hernancasciari.com/libros/).\n",
    "\n",
    "## Estructura del Análisis\n",
    "1. Carga y preparación de datos\n",
    "2. Limpieza y preprocesamiento de texto\n",
    "3. Creación de bag of words\n",
    "4. Análisis de frecuencias\n",
    "5. Visualizaciones y análisis temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5ccWaWUvERF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.- CARGAMOS LOS DATOS A ANALIZAR\n",
    "\n",
    "En vez de conseguir los libros scrapeando la página vamos a cargarlos desde la carpeta de la materia. Son 12 archivos pickleados con terminación .txt correspondientes a los cuentos subidos a su blog en los anios de 2004 a 2015.\n",
    "\n",
    "A pesar de estar en formato .txt los archivos fueron *pickleados* por lo que debemos cargarlos usando la librería pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dqBfaGncEGd",
    "outputId": "52e86434-7edc-4b77-c097-0c85869513cd"
   },
   "outputs": [],
   "source": [
    "# MONTAMOS EL DRIVE PARA CARGAR LOS ARCIVOS\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q46xXH6cexP8"
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6XNqDMAZR3W"
   },
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "import pickle # Utilizada para guardar (serializar) y cargar (deserializar) objetos de Python en/desde archivos binarios.\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 150) # Para que las columnas del dataframe muestren hasta 150 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSZJjIejfD4g"
   },
   "outputs": [],
   "source": [
    "# DEFINIMOS UNA LISTA PARA REFERENCIAR LOS AÑOS DE LAS PUBLICACIONES\n",
    "anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS UN DICCIONARIO PARA ALMACENAR DATOS\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAXjuEpvgdv8",
    "outputId": "c1d01268-6a05-4fdf-bd4a-daf5c0c4aa38"
   },
   "outputs": [],
   "source": [
    "# CARGAMOS CADA ARCHIVO TXT EN FORMATO BINARIO (SERIALIZADO)\n",
    "for anio in anios:\n",
    "    nombre_archivo = path + anio + '.txt'\n",
    "    try:\n",
    "        with open(nombre_archivo, \"rb\") as archivo:\n",
    "            data[anio] = pickle.load(archivo)\n",
    "        print(f\"Archivo {anio}.txt cargado correctamente\")\n",
    "    except:\n",
    "        print(f\"No se pudo cargar el archivo {anio}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0Ny6LcomlLZ",
    "outputId": "2dc1ab21-caeb-46f7-9a53-f22f13a4a552"
   },
   "outputs": [],
   "source": [
    "# VALIDAMOS LA CARGA DE LOS ARCHIVOS EN EL DICCIONARIO. EN ESTE CASO, VERIFICAMOS LAS KEYS\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TO7Qj-CYIFY8",
    "outputId": "893ea6f1-07c1-40d1-a774-a0785cb012d1"
   },
   "outputs": [],
   "source": [
    "# VALIDAMOS LOS PRIMEROS 100 CARACTERES PARA CADA UNO\n",
    "print(data['2004'][0:100])\n",
    "print(data['2005'][0:100])\n",
    "print(data['2006'][0:100])\n",
    "print(data['2007'][0:100])\n",
    "print(data['2008'][0:100])\n",
    "print(data['2009'][0:100])\n",
    "print(data['2010'][0:100])\n",
    "print(data['2011'][0:100])\n",
    "print(data['2012'][0:100])\n",
    "print(data['2013'][0:100])\n",
    "print(data['2014'][0:100])\n",
    "print(data['2015'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBSjyj0svbLA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NOTA DE IMPLEMENTACION DE PYTHON\n",
    "\n",
    "Al intentar crear un DataFrame utilizando un diccionario donde las llaves son valores escalares (es decir, valores únicos y no iterables, como números o cadenas de texto simples), se produce un error porque pandas espera que cada llave tenga un iterable como valor (como listas o arrays).\n",
    "\n",
    "Por ejemplo, si tenemos el siguiente diccionario:\n",
    "\n",
    "data = {\n",
    "    'fruta1': 'manzana',\n",
    "    'fruta2': 'banana'\n",
    "}\n",
    "\n",
    "Aquí, 'fruta1' y 'fruta2' son claves escalares y sus respectivos valores son cadenas simples. Al intentar crear un DataFrame, obtendremos un error porque pandas no puede interpretar varios registros a partir de este formato.\n",
    "\n",
    "Para solucionarlo, convertimos los valores del diccionario en listas, creando un nuevo diccionario así:\n",
    "\n",
    "data_combined = {\n",
    "    'fruta1': ['manzana'],\n",
    "    'fruta2': ['banana']\n",
    "}\n",
    "\n",
    "Ahora, cada valor es una lista (iterable), lo que permite que pandas construya un DataFrame correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ga9PGug7mpsj",
    "outputId": "d10efcfc-1e56-48af-c198-ceec93da1ec0"
   },
   "outputs": [],
   "source": [
    "# RETORNA UNA VISTA DE TODOS LOS PARES KEY-VALUE DEL DICCIONARIO.\n",
    "data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-rqzooVmwbz",
    "outputId": "0ce540bc-53f7-4dec-90d4-9cf2a9f6767c"
   },
   "outputs": [],
   "source": [
    "# NUEVO DICCIONARIO PASANDO LOS VALORES DE LAS KEY A LISTAS\n",
    "data_combined = {key: [value] for (key, value) in data.items()}\n",
    "\n",
    "# CREAMOS UN DATAFRAME\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose() # transponemos para que tenga las dimensiones correctas\n",
    "data_df.columns = ['transcript']                            # renombramos las columnas (en este caso 1)\n",
    "data_df = data_df.sort_index()                              # ordenamos los indices de forma creciente\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kmZpLW6m0S2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.- LIMPIEZA DE DATOS\n",
    "\n",
    "Ahora aplicaremos algunos de los filtros de limpieza que se suelen usar para poder tratar el texto:\n",
    "\n",
    "* Pasar texto a minúsculas\n",
    "* Quitar signos de puntuación (interrogación, y otros símbolos)\n",
    "* Quitar espacios extra, cambio de carro, tabulaciones\n",
    "\n",
    "Para eso vamos a ultilizar las librerías \"re\" y \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoLtRgtlm27A"
   },
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS QUE NECESITAMOS\n",
    "import re       # libreria de expresiones regulares\n",
    "import string   # libreria de cadena de caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wec7Q2pavw0_"
   },
   "source": [
    "RECORDAMOS: Expresiones regulares usan '\\\\' para formas especiales, ej: '\\n' es salto de linea. si queremos imprimir una cadena de caracteres que sea '\\\\' seguido de 'n' se usa la notacion de cadena *raw* en Python que es poner una 'r' antes de la cadena de caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMGiPjmtvuH2"
   },
   "source": [
    "Dicho esto, para encontrar coincidencias en strings en python se utiliza la libreria [re](https://docs.python.org/es/3/library/re.html) de expresiones regulares.\n",
    "\n",
    "Hay muchisimas expresiones y caracteres especiales que sirven para trabajar con texto. Les invito a buscar en la documentacion mas informacion. Vamos a mostrar aca las mas usuales:\n",
    "\n",
    "  \" . \": coincide con cualquier caracter excepto con una nueva línea\n",
    "\n",
    "  \"\\*\": Hace que el RE resultante coincida con 0 o más repeticiones del RE precedente, tantas repeticiones como sean posibles. ab* coincidirá con “a”, “ab” o “a” seguido de cualquier número de “b”.\n",
    "\n",
    "  ?: Hace que la RE resultante coincida con 0 o 1 repeticiones de la RE precedente. ab? coincidirá con “a” o “ab”.\n",
    "\n",
    "  +: Hace que la RE resultante coincida con 1 o más repeticiones de la RE precedente. ab+ coincidirá con “a” seguido de cualquier número distinto de cero de “b”; no coincidirá solo con “a”.\n",
    "\n",
    "  [ ]: Se utiliza para indicar un conjunto de caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcAOXVBKv3z2"
   },
   "source": [
    "También se utiliza [string](https://docs.python.org/es/3/library/string.html) que tiene metodos que permiten acceder a distintos tipos de caracteres. Un ejemplo es acceder a los signos de puntuacion con `string.punctuation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BLZX9OFm8Og",
    "outputId": "28d26dd4-2f38-4d29-d988-05a803f7abba"
   },
   "outputs": [],
   "source": [
    "# CARACTERES QUE NECESITAMOS ELIMINAR\n",
    "print(string.punctuation) # Devuelve todos los caracteres de puntuación comunes como una cadena.\n",
    "print(re.escape(string.punctuation)) # La función escape() del módulo re se utiliza para \"escapar\" todos los caracteres especiales de string.punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JpGvD7YnAyL"
   },
   "outputs": [],
   "source": [
    "# DEFINIMOS UNA FUNCION DE LIMPIEZA\n",
    "# LA FUNCION RECIBE UN TEXTO Y DEVUELVE EL MISMO TEXTO SIN SIGNOS\n",
    "def clean_text_round1(text):\n",
    "    # CONVERTIMOS TODO A MINÚSCULAS.\n",
    "    text = text.lower()\n",
    "    # REEMPLAZAMOS TEXTO ENTRE CORCHETES POR ESPACIOS EN BLANCO.\n",
    "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
    "    # REEMPLAZAMOS SIGNOS DE PUNTUACION POR ESPACIOS EN BLANCO (%s -> \\S+ es cualquier caracter que no sea un espacio en blanco).\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # REMUEVE PALABRAS QUE CONTIENEN NUMEROS.\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnNlgjBuJ3uA"
   },
   "outputs": [],
   "source": [
    "# Defino una funcion anonima que al pasarle un argumento devuelve el resultado de aplicarle la funcion anterior a este mismo argumento\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "# Dataframe que resulta de aplicarle a las columnas la funcion de limpieza\n",
    "data_clean = pd.DataFrame(data_df.transcript.apply(round1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYYFTyTznD53",
    "outputId": "1c7cd16b-8ef6-4952-c039-377fb44e5ba6"
   },
   "outputs": [],
   "source": [
    "print(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "ysMK91Y3nHUN",
    "outputId": "b06953a3-dfa4-4f82-98b6-610e34dddc3f"
   },
   "outputs": [],
   "source": [
    "data_clean.transcript[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXsAcVyQnhSf"
   },
   "outputs": [],
   "source": [
    "# DEFINIMOS OTRA FUNCION DE LIMPIEZA\n",
    "# LA FUNCION RECIBE UN TEXTO Y DEVUELVE EL MISMO TEXTO SIN COMILLAS, PUNTOS SUSPENSIVOS, <<, >>\n",
    "def clean_text_round2(text):\n",
    "    text = re.sub('[‘’“”…«»]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmwkj6Jy8gNR"
   },
   "outputs": [],
   "source": [
    "round2 = lambda x: clean_text_round2(x)\n",
    "\n",
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZTzSCLSnit5"
   },
   "outputs": [],
   "source": [
    "# SERIALIZAMOS EL RESULTADO Y GUARDAMOS EL CORPUS FINAL A TRATAR\n",
    "# data_df.to_pickle(\"/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/corpus.pkl\")\n",
    "data_df.to_pickle(\"C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7iDYVEpnp0u",
    "outputId": "820dc80e-3cee-494e-aa98-b0661082876f"
   },
   "outputs": [],
   "source": [
    "print(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "S41fYKqmn-da",
    "outputId": "400257f3-683a-44bb-fa1e-47f335b50477"
   },
   "outputs": [],
   "source": [
    "data_clean.transcript[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN2LqXDAoFNo",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.- CREAMOS EL “BAG OF WORDS”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "# CountVectorizer: Convierte un conjunto de textos en vectores numéricos, donde cada columna representa una palabra del vocabulario y cada fila \n",
    "# representa un texto. Es lo que se llama una bolsa de palabras (bag of words): solo cuenta cuántas veces aparece cada palabra en el texto \n",
    "# (sin importar el orden).\n",
    "from sklearn.feature_extraction.text import CountVectorizer # CREAMOS LA MATRIZ DE PALABRAS\n",
    "# NLTK es una libreria que ofrece herramientas para:\n",
    "# * Tokenización (dividir texto en palabras, frases, oraciones)\n",
    "# * Lematización y stemming\n",
    "# * Etiquetado gramatical (POS tagging)\n",
    "# * Extracción de entidades\n",
    "# * Corpus integrados (textos de ejemplo)\n",
    "# *Modelos de lenguaje sencillos\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords') # DESCARGAMOS EL MODULO \"stopwords\"\n",
    "lines = nltk.corpus.stopwords.words('spanish') # ASIGNAMOS LAS \"stopwords\" DEL ESPAÑOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UR8bG14oNam",
    "outputId": "7c7119d6-2d8a-4cd3-ca3a-e48b8afa1385"
   },
   "outputs": [],
   "source": [
    "# VISUALIZAMOS EL TIPO DE VARIABLE (LISTA) Y CANTIDAD DE PALABRAS\n",
    "print(type(lines))\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS QUE CONTAMOS CON LAS STOPWORDS DEL ESPAÑOL\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h9lYaHAwFku"
   },
   "source": [
    "Ahora vamos a crear la matriz de terminos-documentos. Para ello hay una funcion de scikit-learn que si le das una secuencia de items de tipo string o byte hace exactamente eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3B7cFlxoQYA"
   },
   "outputs": [],
   "source": [
    "# INSTANCIAMOS UNA VARIABLE DE TIPO \"CountVectorizer\" Y LE INDICAMOS QUE CUENTA AHORA CON UN \"stop_words\" DEFINIDO\n",
    "count_vect = CountVectorizer(stop_words=lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAMOS EL TEXTO EN DATOS NUMÉRICOS USANDO \"CountVectorizer\" PARA CONVERTIR LOS TEXTOS DE \"data_clean.transcript\" EN UNA MATRIZ.\n",
    "data_cv = count_vect.fit_transform(data_clean.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAMOS UN DATAFRAME TRANSFORMANDO CON LA MATRIZ QUE GENERAMOS ANTES DONDE:\n",
    "# 1.- LAS COLUMNAS SON LAS PALABRAS\n",
    "# 2.- LAS FILAS SON LOS DOCUMENTOS\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=count_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASIGNAMOS LOS INDICES DEL DATAFRAME ANTERIOR\n",
    "data_dtm.index = data_clean.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YStWpDaDK2y7",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "En el contexto del procesamiento de texto y el análisis de datos, la matriz esparsa es una herramienta fundamental. Esta matriz surge del deseo de representar documentos como vectores, donde cada dimensión (o columna) se corresponde con una palabra del vocabulario.\n",
    "\n",
    "La razón por la que utilizamos matrices esparsas es que en la mayoría de los casos, un documento contiene solo un pequeño subconjunto del vocabulario total. Por ejemplo, si tenemos un corpus con miles de palabras pero un documento particular solo utiliza unas pocas, la matriz resultante de la representación será mayormente llena de ceros.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Supongamos que tenemos 3 documentos y un vocabulario de 4 palabras:\n",
    "\n",
    "Documentos:\n",
    "\n",
    "* 1: \"manzana\"\n",
    "* 2: \"banana\"\n",
    "* 3: \"manzana banana\"\n",
    "\n",
    "Vocabulario: [\"manzana\", \"banana\", \"uva\", \"pera\"]\n",
    "\n",
    "La matriz resultante sería:\n",
    "\n",
    "        manzana  banana  uva  pera\n",
    "     \n",
    "     1       1       0    0     0   # Documento 1\n",
    "     \n",
    "     2       0       1    0     0   # Documento 2\n",
    "     \n",
    "     3       1       1    0     0   # Documento 3\n",
    "\n",
    "\n",
    "Esta representación eficiente tiene varias ventajas:\n",
    "\n",
    "1. **Ahorro de espacio**: Almacenar solo los valores no cero y sus posiciones ahorra considerablemente espacio en disco y memoria.\n",
    "\n",
    "2. **Rendimiento**: Muchas operaciones matemáticas en matrices esparsas se pueden hacer de manera más rápida y eficiente, lo que es crucial para manejar grandes volúmenes de texto.\n",
    "\n",
    "Como resultado, al ajustar el modelo (como `CountVectorizer` o `TF-IDF`), generamos una matriz esparsa que nos permite realizar análisis más complejos sin perder rendimiento ni eficiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS LA DEFINICIÓN DE LA MATRIZ ESPARZA/DISPERSA: ES UNA MATRIZ QUE CONTIENE MUCHISIMOS CEROS Y MUY POCOS VALORES DISTINTO DE CERO.\n",
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "z1Qvmgbfomi_",
    "outputId": "a3795f9b-5acc-4fcf-d94b-a3d633980850"
   },
   "outputs": [],
   "source": [
    "# VISUALIZAMOS LA MATRIZ EN FORMATO DATAFRAME (12 FILAS Y 29713 COLUMNAS)\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVbMfc_wwM-n"
   },
   "source": [
    "----\n",
    "Vemos que hay palabras que no estan en el idioma espaniol como **českomoravský**. Dependiendo la informacion que queramos obtener con estos datos entonces tal vez es conveniente filtrar este tipo de palabras. Como lo que vamos a analizar en esta notebook es el vocabulario del escritor, en particular el numero de palabras unicas, entonces las vamos a dejar.\n",
    "\n",
    "**nltk** tiene [corpus](https://www.nltk.org/book/ch02.html) que se pueden cargar y utilizar para filtrar palabras de distintos idiomas o nombres propios.\n",
    "\n",
    "```\n",
    "# Ejemplo de palabras en ingles\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "```\n",
    "\n",
    "Ojo! que hay palabras que estan en ingles pero no estan en un *diccionario*, como expresiones tipicas de chats, algunos nombres propios o abreviaturas. Entonces hay que tener cuidado cuando pasamos de mayusculas a minusculas. Si las queremos filtrar entonces tenemos que ser cuidadosxs con el corpus que usamos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfS6EnZhwRqh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.- GUARDAMOS TODOS LOS DATOS GENERADOS (SERIALIZADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUARDAMOS EL DATAFRAME\n",
    "data_dtm.to_pickle(\"C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GURDAMOS EL TEXTO LIMPIO SOBRE EL CUAL TRABAJAMOS\n",
    "data_clean.to_pickle('C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\data_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUARDAMOS EL OBJETO CountVectorize\n",
    "pickle.dump(count_vect, open(\"C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvBAJHZLwYHX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.- ANÁLISIS EXPLORATORIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 CARGAMOS EL DATAFRAME QUE GUARDAMOS ANTERIORMENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FIA5B2z5pUFP",
    "outputId": "690be42b-beb0-40c2-aaab-7cc9c5cca63a"
   },
   "outputs": [],
   "source": [
    "# data = pd.read_pickle('/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/dtm.pkl')\n",
    "data = pd.read_pickle('C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\dtm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSPONEMOS LA MATRIZ PARA TENER UNA EN FORMATO \"terminosXdocumentos\"\n",
    "data = data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAMOS LOS PRIMEROS 10 REGISTROS\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 BUSCAMOS LAS PALABRAS MAS USADAS POR AÑO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2MuILorpcvM"
   },
   "outputs": [],
   "source": [
    "# CREAMOS UN DICCIONARIO VACÍO\n",
    "top_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECORRO LA MATRIZ POR CADA COLUMNA (AÑO)\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30) # Ordeno las filas en forma decreciente y me quedo con las 30 palabras mas usadas\n",
    "    top_dict[c]= list(zip(top.index, top.values))       # le asigno el año a la key del diccionario y como valor una tupla con la palabra y su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_dict)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAMOS LAS 15 PALABRAS MAS FRECUENTES POR CADA AÑO\n",
    "for anio, top_words in top_dict.items():\n",
    "    print(anio) # IMPRIMO LAS KEYS\n",
    "    print(', '.join([word for word, count in top_words[0:14]])) # MOSTRAMOS LAS PALABRAS EN ORDEN DECRECIENTE SEGUN FRECUENCIA Y SEPARADAS CON ESPACIO Y COMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 AGREGAMOS LAS STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQbeHTFewmwp"
   },
   "source": [
    "Vemos que hay nombres propios, numeros escrito con letras y algunas palabras como el adverbio \"tan\", \"si\", \"cada\", etc. que no nos da informacion por si solo. Debemos filtrar estas palabras.\n",
    "\n",
    "Tambien vemos que hay palabras que se repiten dentro de las mas frecuentes de cada anio en todos los *anios* que tambien queremos filtrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4-T-PEewpaN"
   },
   "source": [
    "En este caso vamos a unir las 12 listas de palabras mas usadas en un nuevo ranking y de esas, tomaremos las \"mas usadas\" para agregar al listado de Stop Words. Es decir, si una palabra aparecio en el top 30 mas de la mitad de las veces (en 7 anios o mas) entonces la considero una stop word.\n",
    "\n",
    "Vamos a usar la funcion Counter de la libreria collections que dada una lista de strings nos devuelve la cantidad de repeticiones de cada elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LA LIBRERIA\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS UNA LISTA CON LAS 30 PALABRAS MAS USADAS DE CADA AÑO\n",
    "words = []\n",
    "for anio in data.columns:\n",
    "    top = [word for (word, count) in top_dict[anio]] # armo una lista quedandome solo con las palabras del diccionario que creamos antes\n",
    "    for t in top:\n",
    "        words.append(t)                              # agrego cada palabra a la lista vacia, por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando el metodo .most_common() obtengo las palabras mas usadas, y la cantidad de veces que fue agregada a la lista words\n",
    "print(Counter(words).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Z6muxEdpmyc",
    "outputId": "98be3bc1-d74e-4370-e3f9-54a23b04742f"
   },
   "outputs": [],
   "source": [
    "# Creo una lista de nuevas stop words considerando como stop words a palabras que recibieron mas de la mitad de las cuentas posibles (12 anios)\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "print(add_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WJUn7XPwwTL"
   },
   "source": [
    "#### 5.4 ACTUALIZAMOS LA \"BAG OF WORDS\"\n",
    "\n",
    "Ahora quitaremos las \"stop_words\" de nuestro dataset. Para esto hacemos lo siguiente:\n",
    "\n",
    "1.- Usamos el listado de \"stop_words\" en espaniol de nltk, el que generamos recién y uno adicional que tiene en cuenta las observaciones que palabras fueron las mas frecuentes.\n",
    "\n",
    "2.- Luego, volveremos a cargar los datos limpios y la filtramos aplicando las dos nuevas listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LA LIBRERIA\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECUPERAMOS LOS DATOS LIMPIOS QUE GUARDAMOS EN LA SERIALIZACIÓN\n",
    "data_clean = pd.read_pickle('C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\data_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos la lista de stop words \"lines\" las nuevas stop words\n",
    "for pal in add_stop_words:\n",
    "    lines.append(pal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la lista que tiene algunos nombres propios que aparecieron entre las palabras mas frecuentes y otro tipo de palabras\n",
    "more_stop_words=['alex','lucas','andrés','mirta','tres','primer','primera','dos','uno','veces', 'así', 'luego', 'quizá','cosa','cosas','tan','asi','andres','todas','sólo','jesús','pablo','pepe']\n",
    "for pal in more_stop_words:\n",
    "    lines.append(pal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreamos la matriz de documentos y terminos pero ustando la nueva lista \"mejorada\" de stopwords\n",
    "count_vect = CountVectorizer(stop_words=lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv = count_vect.fit_transform(data_clean.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_feature_names_out() instead of get_feature_names()\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=count_vect.get_feature_names_out())\n",
    "data_stop.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo guardamos en formato pickle\n",
    "pickle.dump(count_vect, open(\"C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\cv_stop.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stop.to_pickle(\"C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\dtm_stop.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ggzYz3eqnPU"
   },
   "source": [
    "#### 5.5 ARMAMOS LA \"NUBE DE PALABRAS\"\n",
    "Queremos ver “que tenia Hernan Casciari en su cabeza” entre 2004 y 2015 en sus cuentos usando WordClouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el objeto WordCloud con determinados parametros y utilizando nuestra lista de stopwords\n",
    "wc = WordCloud(stopwords=lines, background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16,12] # tamanio de los plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cada anio creo un WC\n",
    "for index, anio in enumerate(data.columns):\n",
    "    wc.generate(data_clean.transcript[anio])  # aca le pido que genere los WC a partir del texto de cada anio\n",
    "    plt.subplot(4, 3, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(anios[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZHMKjqdqxlr"
   },
   "source": [
    "La hija de Hernan Casciari nacio en el 2004. Se puede tratar de analizar como el nacimiento de su hija influencio su escritura. En el 2007, cuando ella tenia tres anios, vemos que empezo a tener mas importancia la palabra \"padre\" en sus cuentos. Y cuando ella tenia once anios, en el 2015, es la palabra mas frecuente. Siguiendo este analisis se puede pensar que verla crecer afecto enormemente el contenido de sus cuentos.\n",
    "\n",
    "Siguiendo este analisis interpretativo se nota en estas wordclouds algunos topicos importantes que definen su personalidad. Es un escritor que le gusta mucho escribir sobre su vida, por lo que no es de extraniarse que \"libro\", \"revista\" u \"orsai\", su editorial, aparezcan en muchas WCs. Tambien sabemos que es un fanatico del \"futbol\", y que es muy familiero (\"padre\", \"madre\", \"hija\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-znDgNvWq0gJ"
   },
   "source": [
    "#### 5.6 ESTADÍSTICAS DE PALABRAS POR AÑO\n",
    "Ahora sacaremos algunas estadísticas de palabras únicas por año (el tamaño del vocabulario empleado) y el promedio de palabras por artículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "GxJSXBjUq1Hu",
    "outputId": "c06661f5-689c-4b6d-e3a4-f01565922793"
   },
   "outputs": [],
   "source": [
    "#\n",
    "data[data.columns[1]].to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontraremos las palabras unicas por anio identificando los elementos non-zero en la matriz de documentos y terminos\n",
    "unique_list = []\n",
    "for anio in data.columns:\n",
    "    uniques = data[anio].to_numpy().nonzero()[0].size # tengo que transformar a un array para aplicar la funcion nonzero\n",
    "    unique_list.append(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un nuevo dataframe con el numero de palabras unicas por anio\n",
    "data_words = pd.DataFrame(list(zip(anios, unique_list)), columns=['Anio', 'unique_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_unique_sort = data_words.sort_values(by='unique_words')\n",
    "data_unique_sort = data_words # sin ordenar\n",
    "print(data_unique_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El numero de post por anio, esto es dato\n",
    "posts_per_year = [50, 27, 18, 50, 42, 22, 50, 33, 31, 17, 33, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YUBE_DTq33g",
    "outputId": "52e178bd-d771-4c84-b061-f7647d32dafb"
   },
   "outputs": [],
   "source": [
    "# Encuentro el numero total de palabras por anio\n",
    "total_list = []\n",
    "for anio in data.columns:\n",
    "    totals = sum(data[anio])\n",
    "    total_list.append(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ux9Abdbjq9p8",
    "outputId": "47206282-3d36-428f-89ee-7ce84392ade6"
   },
   "outputs": [],
   "source": [
    "# Agrego las columnas al dataframe\n",
    "data_words['total_words'] = total_list\n",
    "data_words['posts_per_year'] = posts_per_year\n",
    "data_words['words_per_posts'] = data_words['total_words'] / data_words['posts_per_year']\n",
    "\n",
    "data_wpm_sort = data_words #sin ordenar\n",
    "print(data_wpm_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8-WoimjrE8t"
   },
   "source": [
    "#### 5.7 VISUALIZACIÓN DE LA TABLA\n",
    "Veamos los datos en gráfico de barras horizontales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "L6fpK6pjrCQ6",
    "outputId": "62ea4863-44e4-4d27-91e8-772b7235260b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "y_pos = np.arange(len(data_words))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.barh(y_pos,posts_per_year, align='center')\n",
    "plt.yticks(y_pos, anios)\n",
    "plt.title('# de Posts', fontsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n",
    "plt.yticks(y_pos, data_unique_sort.Anio)\n",
    "plt.title('# palabras unicas', fontsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(y_pos, data_wpm_sort.words_per_posts, align='center')\n",
    "plt.yticks(y_pos, data_wpm_sort.Anio)\n",
    "plt.title('# palabras por post', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOpYdhbWxIdH"
   },
   "source": [
    "Y hagamos una comparativa de frecuencia de uso de algunas palabras (se pueden elegir cualquiera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "ACYhmQvXrM4m",
    "outputId": "81e13465-b3fe-4482-e4af-ca971707c58c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = \"C:\\\\Users\\\\ddulcich\\\\Documents\\\\PERSONAL\\\\IFTS24\\\\16.-TECNICAS DE PROCESAMIENTO DE HABLA\\\\TPS\\\\libros\\\\\"\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "#wordlists.fileids() # con esto listamos los archivos del directorio\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "        (word,genre)\n",
    "        for genre in anios\n",
    "        for w in wordlists.words(genre + '.txt')\n",
    "        for word in ['casa','futbol','tiempo','vida']\n",
    "        if w.lower().startswith(word) )\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iN0ljyorddJ"
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "Repasemos lo que hicimos y que resultados sacamos:\n",
    "\n",
    "* Limpiamos los textos, quitamos caracteres que no utilizamos y creamos un listado de stop_words (palabras para omitir)\n",
    "\n",
    "* Exploración de datos:\n",
    "Realizamos estadísticas básicas, como el vocabulario usado, cantidad de palabras por año y promedio por posts.\n",
    "\n",
    "* Creamos Nubes de Palabras por año ya que es una manera de visualizar textos y frecuencia de termino a lo largo de los anios (similar a [Google Ngrams Viewer](https://books.google.com/ngrams)) falta dividir por la cantidad total de palabras de cada anio"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
