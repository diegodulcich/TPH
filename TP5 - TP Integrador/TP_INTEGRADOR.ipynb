{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcNe_-frZZ10",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TP5 - TRABAJO PRÁCTICO INTEGRADOR - TEXT MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRIB2XaduKHq"
   },
   "source": [
    "# Introducción al Text Mining \n",
    "\n",
    "Es el proceso de extraer información útil y conocimiento a partir de grandes cantidades de texto no estructurado.\n",
    "Cuando hablamos de texto no estructurado, nos referimos a cosas como:\n",
    "* Noticias\n",
    "* Comentarios en redes sociales\n",
    "* Opiniones de usuarios\n",
    "* Emails, documentos, libros, etc.\n",
    "* \n",
    "Este tipo de texto no tiene una estructura clara como una tabla de Excel con lo cual se aplican técnicas de procesamiento de lenguaje natural (NLP), estadística e inteligencia artificial para encontrar patrones, relaciones y significado.\n",
    "\n",
    "¿Qué cosas se hacen con text mining?\n",
    "* Contar frecuencia de palabras (como lo que estás haciendo ahora)\n",
    "* Detectar temas o tópicos\n",
    "* Análisis de sentimiento (¿el texto es positivo o negativo?)\n",
    "* Extracción de entidades (nombres, lugares, organizaciones)\n",
    "* Clasificación de texto (por ejemplo, spam vs no spam)\n",
    "* Resumen automático\n",
    "* Clustering (agrupar textos similares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.- Implementación de \"WEB SCRAPING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El web scraping (o raspado web), es una técnica que utiliza software (bots, crawlers o spider) para extraer datos de sitios web de forma automatizada. Estos programas simulan la navegación web y pueden identificar, extraer y almacenar información específica, como texto, números, imágenes o incluso datos de formularios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.- DEFINIMOS URL Y CONTENIDO A PROCESAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS \n",
    "import requests # LIBRERIA PARA HACER SOLICITUDES HTTP\n",
    "from bs4 import BeautifulSoup # LIBRERIA PARA PERMITIR EXTRAER INFORMACION DE PAGINAS WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS LAS URLS CON LA CUALES VAMOS A TRABAJAR\n",
    "url1 = \"https://vueltaolimpica.com/index.php/category/maratones/page/1/\"\n",
    "url2 = \"https://vueltaolimpica.com/index.php/category/maratones/page/2/\"\n",
    "url3 = \"https://vueltaolimpica.com/index.php/category/maratones/page/3/\"\n",
    "url4 = \"https://vueltaolimpica.com/index.php/category/maratones/page/4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN LA VARIABLE CONTENIDO RECIBIMOS LA RESPUESTA DE LA SOLICITUD A LA PAGINA WEB QUE INVOCAMOS\n",
    "# COMO LA RESPUESTA DEL SERVIDOR WEB PUEDE CONTENER DIFERENTES TIPOS DE CONTENIDO (HTML, XML, JSON, ETC) INVOCAMOS AL MÉTODO \"TEXT\"\n",
    "# PARA QUE TODO EL CONTENIDO DEVUELTO SEA TRATADO COMO TEXTO\n",
    "contenido1 = requests.get(url1).text\n",
    "contenido2 = requests.get(url2).text\n",
    "contenido3 = requests.get(url3).text\n",
    "contenido4 = requests.get(url4).text\n",
    "# CONCATENAMOS EL CONTENIDO EN UNA SOLA VARIABLE\n",
    "contenido = contenido1 + contenido2 + contenido3 + contenido4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVOCAMOS A LA LIBRERIA \"BEATIFULSOAP\"\n",
    "# LE PASAMOS COMO PARÁMETRO EL TEXTO A PROCESAR Y LE INDICAMOS QUE DEBE TRATARLO/PARSEARLO COMO UN CONTENIDO HTML\n",
    "soup = BeautifulSoup(contenido, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSCAMOS LA ETIQUETA <h1> CON CLASE \"page-title\"\n",
    "titulo = soup.find('h1', class_='page-title')\n",
    "# EXTRAEMOS EL TEXTO (QUITAMOS LAS ETIQUETAS)\n",
    "if titulo:\n",
    "    texto = titulo.get_text(strip=True)  # ELIMINAMOS ESPACIOS\n",
    "    print(f\"Título:\", texto)\n",
    "# BUSCAMOS LA ETIQUETA <meta> CON property=\"og:description\"\n",
    "meta_tag = soup.find('meta', property='og:description')\n",
    "# EXTRAEMOS EL CONTENIDO DEL ATRIBUTO \"content\"\n",
    "if meta_tag and meta_tag.has_attr('content'):\n",
    "    descripcion = meta_tag['content']\n",
    "    print(f\"Descripción:\", descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS UNA LISTA VACIA QUE CONTENDRÁ LAS URLS A PROCESAR\n",
    "urls_validas = []\n",
    "# RECORREMOS LA LISTA DE URLS BUSCANDO LAS QUE SEAN \"href\" Y QUE EN PARTE DE SU URL CONTENGAN \"20\"\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if \"20\" in link['href']:\n",
    "        urls_validas.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS LAS URLS QUE CUMPLEN CON LA CONDICION \"20xx\"\n",
    "# NOTA: OBSERVAMOS URLS DUPLICADAS Y ALGUNAS QUE NO SE CORRESPONDEN CON TEXTO A PROCESAR CON LO CUAL DEBEMOS LIMPIARLAS\n",
    "urls_validas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAMOS CUANTAS SE RETORNARON (PARA IR OBSERVANDO EL PROCESO DE LIMPIEZA)\n",
    "len(urls_validas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUITAMOS LAS DUPLICADAS\n",
    "urls_validas = list(set(urls_validas))\n",
    "# DEFINIMOS UNA LISTA POR COMPRENSION QUE FILTRA LA LISTA \"urls_validas\" DEJANDO SOLO LOS ELEMENTOS QUE CONTIENEN EL TEXTO INDICADO.\n",
    "urls_validas = [url for url in urls_validas if 'https://vueltaolimpica.com/index.php/' in url]\n",
    "# VISUALIZAMOS LAS URLS ORDENADAS POR AÑO/MES/DIA\n",
    "sorted(urls_validas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS EL RESULTADO\n",
    "sorted(urls_validas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAMOS CUANTAS SE RETORNARON (PARA IR OBSERVANDO EL PROCESO DE LIMPIEZA)\n",
    "len(urls_validas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAMOS LAS LISTAS VACIAS PARA CONTENER A LAS URLS DEL AÑO CORRESPONDIENTE\n",
    "urls_2023 = []\n",
    "urls_2024 = []\n",
    "urls_2025 = []\n",
    "for url in urls_validas:\n",
    "    if '/2023/' in url:\n",
    "        urls_2023.append(url)\n",
    "    elif '/2024/' in url:\n",
    "        urls_2024.append(url)\n",
    "    elif '/2025/' in url:\n",
    "        urls_2025.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS QUE LAS URLS ESTÁN CORRECTAMENTE ASOCIADAS A CADA LISTA\n",
    "urls_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS QUE LAS URLS ESTÁN CORRECTAMENTE ASOCIADAS A CADA LISTA\n",
    "urls_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS QUE LAS URLS ESTÁN CORRECTAMENTE ASOCIADAS A CADA LISTA\n",
    "urls_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTENEMOS EL TEXTO A TRABAJAR PARA 2023\n",
    "text_2023 = \" \"\n",
    "for item in urls_2023:\n",
    "  texto_html = requests.get(item).text\n",
    "  soup = BeautifulSoup(texto_html, \"html.parser\")\n",
    "  for paragraphe in soup.find_all(\"p\"):\n",
    "    text_2023 += str(paragraphe.getText())\n",
    "# print(text_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTENEMOS EL TEXTO A TRABAJAR PARA 2024\n",
    "text_2024 = \" \"\n",
    "for item in urls_2024:\n",
    "  texto_html = requests.get(item).text\n",
    "  soup = BeautifulSoup(texto_html, \"html.parser\")\n",
    "  for paragraphe in soup.find_all(\"p\"):\n",
    "    text_2024 += str(paragraphe.getText())\n",
    "# print(text_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTENEMOS EL TEXTO A TRABAJAR PARA 2025\n",
    "text_2025 = \" \"\n",
    "for item in urls_2025:\n",
    "  texto_html = requests.get(item).text\n",
    "  soup = BeautifulSoup(texto_html, \"html.parser\")\n",
    "  for paragraphe in soup.find_all(\"p\"):\n",
    "    text_2025 += str(paragraphe.getText())\n",
    "# print(text_2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.- LIMPIEZA DEL TEXTO A PROCESAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "import re       # libreria de expresiones regulares\n",
    "import string   # libreria de cadena de caracteres\n",
    "from collections import Counter # librería para contar elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARACTERES A ELIMINAR\n",
    "print(string.punctuation) # CARACTERES DE PUNTUACION COMUNES EN UNA CADENA DE TEXTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARACTERES A ELIMINAR\n",
    "print(re.escape(string.punctuation)) # escape() SE UTILIZA PARA \"escapar\" TODOS LOS CARACTERES ESPECIALES DE \"string.punctuation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS UNA FUNCION DE LIMPIEZA\n",
    "def clean_text_round1(text):\n",
    "    # LA FUNCION RECIBE UN TEXTO Y DEVUELVE EL MISMO TEXTO SIN SIGNOS\n",
    "    # ****************************************************************\n",
    "    # NOTA IMPORTANTE:\n",
    "    # PARA SOLUCIONAR ESTE WARNING EN RE: \"SyntaxWarning: invalid escape sequence '\\w'\"\n",
    "    # REEMPLAZAMOS LA LLAMADA:  text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
    "    # POR: text = re.sub(r'\\[.*?¿\\]\\%', ' ', text)\n",
    "    # ****************************************************************\n",
    "    # CONVERTIMOS TODO A MINÚSCULAS.\n",
    "    text = text.lower()\n",
    "    # REEMPLAZAMOS TEXTO ENTRE CORCHETES POR ESPACIOS EN BLANCO.\n",
    "    text = re.sub(r'\\[.*?¿\\]\\%', ' ', text)\n",
    "    # REEMPLAZAMOS SIGNOS DE PUNTUACION POR ESPACIOS EN BLANCO (%s -> \\S+ es cualquier caracter que no sea un espacio en blanco).\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # REMUEVE PALABRAS QUE CONTIENEN NUMEROS.\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS OTRA FUNCION DE LIMPIEZA\n",
    "def clean_text_round2(text):\n",
    "    # LA FUNCION RECIBE UN TEXTO Y DEVUELVE EL MISMO TEXTO SIN COMILLAS, PUNTOS SUSPENSIVOS, <<, >>\n",
    "    text = re.sub('[‘’“”…«»\\xa0]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAMOS EL TAMAÑO DE LOS TEXTOS A PROCESAR ANTES DE INVOCAR LAS FUNCIONES DE LIMPIEZA\n",
    "print(f\"TAMAÑO 2023:\", len(text_2023))\n",
    "print(f\"TAMAÑO 2024:\", len(text_2024))\n",
    "print(f\"TAMAÑO 2025:\", len(text_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2023 = clean_text_round1(text_2023)\n",
    "text_2024 = clean_text_round1(text_2024)\n",
    "text_2025 = clean_text_round1(text_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAMOS EL TAMAÑO DE LOS TEXTOS LUEGO DE INVOCAR LA FUNCION DE LIMPIEZA #1\n",
    "print(f\"TAMAÑO 2023:\", len(text_2023))\n",
    "print(f\"TAMAÑO 2024:\", len(text_2024))\n",
    "print(f\"TAMAÑO 2025:\", len(text_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2023 = clean_text_round2(text_2023)\n",
    "text_2024 = clean_text_round2(text_2024)\n",
    "text_2025 = clean_text_round2(text_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAMOS EL TAMAÑO LUEGO DE INVOCAR LA FUNCION DE LIMPIEZA #2\n",
    "print(f\"TAMAÑO 2023:\", len(text_2023))\n",
    "print(f\"TAMAÑO 2024:\", len(text_2024))\n",
    "print(f\"TAMAÑO 2025:\", len(text_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_palabras_2023 = re.sub(r\"[^\\w]\", \" \", text_2023).split()\n",
    "lista_de_palabras_2024 = re.sub(r\"[^\\w]\", \" \", text_2024).split()\n",
    "lista_de_palabras_2025 = re.sub(r\"[^\\w]\", \" \", text_2025).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contador_2023 = Counter(lista_de_palabras_2023)\n",
    "contador_2024 = Counter(lista_de_palabras_2024)\n",
    "contador_2025 = Counter(lista_de_palabras_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAMOS REPETICION DE PALABRAS PARA CADA TEXTO\n",
    "print(contador_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contador_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAMOS REPETICION DE PALABRAS PARA CADA TEXTO\n",
    "print(contador_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contador_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAMOS REPETICION DE PALABRAS PARA CADA TEXTO\n",
    "print(contador_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contador_2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.- Implementación de \"NLTK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit) es una biblioteca de Python para el procesamiento del lenguaje natural (PLN). Proporciona herramientas y datos para realizar tareas como tokenización, análisis sintáctico, etiquetado de palabras, y más. Es ampliamente utilizada en investigación, desarrollo de aplicaciones de PLN, y análisis de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "import nltk # LIBRERIA PARA PROCESAMIENTO DE LENGUAJE NATURAL\n",
    "from nltk.corpus import stopwords # PALABRAS COMUNES DE UN IDIOMA QUE GENERALMENTE SE ELIMINAN DEL TEXTO A ANALIZAR (\"el\", \"la\", \"de\", \"un\", etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCARGAMOS EL CORPUS DE PALABRAS EN ESPAÑOL DE NTLK\n",
    "nltk.download('stopwords')\n",
    "stopwords_es = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS EL CORPUS DE LAS STOP_WORDS EN ESPAÑOL\n",
    "print(stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS UNA FUNCION PARA QUITAR LAS STOPWORDS\n",
    "def limpiar_texto_sw(texto):\n",
    "  # DEFINIMOS UNA LISTA VACIA QUE LUEGO CONTENERÁ LAS PALABRAS IMPORTANTES A ANALIZAR\n",
    "  palabras_importantes = []\n",
    "  # RECORREMOS LA LISTA DE PALABRAS Y AQUELLAS QUE NO ESTÉN EN \"stopwords_es\" LA AGREGAMOS A LA LISTA\n",
    "  for palabra in texto:\n",
    "    if palabra not in stopwords_es:\n",
    "      palabras_importantes.append(palabra)\n",
    "  return palabras_importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAMOS EL TAMAÑO ANTES DE INVOCAR LA FUNCION QUE CONTIENE LA DEFINICIÓN DE STOPWORDS\n",
    "print(f\"TAMAÑO 2023:\", len(lista_de_palabras_2023))\n",
    "print(f\"TAMAÑO 2024:\", len(lista_de_palabras_2024))\n",
    "print(f\"TAMAÑO 2025:\", len(lista_de_palabras_2025))\n",
    "print(f\"CONTADOR 2023:\", len(contador_2023))\n",
    "print(f\"CONTADOR 2024:\", len(contador_2024))\n",
    "print(f\"CONTADOR 2025:\", len(contador_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APLICAMOS LA FUNCION DE LIMPIEZA QUE CONTIENE LAS STOPWORDS (ESPAÑOL)\n",
    "lista_de_palabras_2023 = limpiar_texto_sw(lista_de_palabras_2023)\n",
    "lista_de_palabras_2024 = limpiar_texto_sw(lista_de_palabras_2024)\n",
    "lista_de_palabras_2025 = limpiar_texto_sw(lista_de_palabras_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contador_2023 = Counter(lista_de_palabras_2023)\n",
    "contador_2024 = Counter(lista_de_palabras_2024)\n",
    "contador_2025 = Counter(lista_de_palabras_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAMOS EL TAMAÑO DESPUES DE INVOCAR LA FUNCION QUE CONTIENE LA DEFINICIÓN DE STOPWORDS\n",
    "print(f\"TAMAÑO 2023:\", len(lista_de_palabras_2023))\n",
    "print(f\"TAMAÑO 2024:\", len(lista_de_palabras_2024))\n",
    "print(f\"TAMAÑO 2025:\", len(lista_de_palabras_2025))\n",
    "print(f\"CONTADOR 2023:\", len(contador_2023))\n",
    "print(f\"CONTADOR 2024:\", len(contador_2024))\n",
    "print(f\"CONTADOR 2025:\", len(contador_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDEFINIMOS LA FUNCION PARA LIMPIAR TEXTO #4\n",
    "# *****************************************************************************************************\n",
    "# NOTA: EN ESTA FUNCION AGREGAMOS UNA LISTA DE PALABRAS MANDATORIAS (DEFINIDAS POR NOSOTROS) A ELIMINAR\n",
    "# *****************************************************************************************************\n",
    "def limpiar_texto_swm(texto):\n",
    "  # DEFINIMOS UNA LISTA DE PALABRAS PERSONALIZADAS A ELIMINAR (swm = STOPWORDS MEJORADO)\n",
    "  palabras_a_eliminar = ['correo', 'electrónico', 'web', 'dijo', 'así', 'aunque', 'sino', 'luego', 'pues', 'mientras', 'después', 'antes', 'porque', \n",
    "                         'cuando', 'cómo', 'donde', 'cap', 'capítulo', 'verso', 'canto', 'á', 'ó', 'si', 'ser', 'bien', 'vez', 'juan', 'hs', 'dos',\n",
    "                         'nombre', 'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'a', 'ante', 'bajo', 'cabe', 'con', 'contra', 'de', 'desde',\n",
    "                         'en', 'entre', 'hacia', 'hasta', 'para', 'por', 'según', 'sin', 'so', 'sobre', 'tras', 'durante', 'mediante', 'y', 'o', 'u', \n",
    "                         'ni', 'que', 'como', 'cuando', 'donde', 'qué', 'cuál', 'cuáles', 'quién', 'quienes', 'cuyo', 'cuya', 'cuyos', 'cuyas', 'pero', \n",
    "                         'aunque', 'porque', 'si', 'no', 'sí', 'más', 'menos', 'también', 'ya', 'aún', 'solo', 'se', 'le', 'lo', 'la', 'me', 'te', \n",
    "                         'nos', 'os', 'su', 'sus', 'mi', 'mis', 'tu', 'tus', 'nuestro', 'nuestra', 'vuestro', 'vuestra', \n",
    "                         'Ana', 'María', 'Laura', 'Lucía', 'Carmen', 'Sofía', 'Isabella', 'Valentina', 'Paula', 'Gabriela',\n",
    "                         'Andrea', 'Rocío', 'Daniela', 'Elena', 'Claudia', 'Julia', 'Carla', 'Sara', 'Ángela', 'Verónica',\n",
    "                         'Natalia', 'Lorena', 'Alicia', 'Beatriz', 'Patricia', 'Silvia', 'Noelia', 'Marta', 'Bianca', 'Camila',\n",
    "                         'Jimena', 'Eva', 'Florencia', 'Adriana', 'Victoria', 'Rosa', 'Manuela', 'Rebeca', 'Lourdes', 'Cristina',\n",
    "                         'Milagros', 'Tamara', 'Marina', 'Pilar', 'Guadalupe', 'Montserrat', 'Inés', 'Leticia', 'Bárbara', 'Alejandra',\n",
    "                         'Juan', 'Luis', 'Carlos', 'Pedro', 'José', 'Andrés', 'Miguel', 'Santiago', 'Mateo', 'Diego',\n",
    "                         'Daniel', 'Pablo', 'Ángel', 'Fernando', 'Alberto', 'Rubén', 'Javier', 'Manuel', 'Antonio', 'Sergio',\n",
    "                         'Jorge', 'Ricardo', 'Francisco', 'Lucas', 'Eduardo', 'Iván', 'Martín', 'David', 'Nicolás', 'Cristian',\n",
    "                         'Tomás', 'Esteban', 'Gabriel', 'Adrián', 'Rodrigo', 'Agustín', 'Emilio', 'Rafael', 'Hugo', 'Bruno',\n",
    "                          'Enrique', 'Salvador', 'Raúl', 'Sebastián', 'Gonzalo', 'Ignacio', 'Fabián', 'Joaquín', 'Vicente', 'Marco'                 \n",
    "                        ]\n",
    "  # DEFINIMOS UNA VARIABLE QUE UNIFICA LAS DOS LISTAS\n",
    "  black_list = stopwords_es.union(palabras_a_eliminar)\n",
    "  # DEFINIMOS UNA LISTA VACIA QUE LUEGO CONTENERÁ LAS PALABRAS IMPORTANTES A ANALIZAR\n",
    "  palabras_importantes = []\n",
    "  # RECORREMOS LA LISTA DE PALABRAS Y AQUELLAS QUE NO ESTÉN EN \"black_list\" LA AGREGAMOS A LA LISTA\n",
    "  for palabra in texto:\n",
    "    if palabra not in black_list:\n",
    "      palabras_importantes.append(palabra)\n",
    "  return palabras_importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_palabras_2023 = limpiar_texto_swm(lista_de_palabras_2023)\n",
    "lista_de_palabras_2024 = limpiar_texto_swm(lista_de_palabras_2024)\n",
    "lista_de_palabras_2025 = limpiar_texto_swm(lista_de_palabras_2025)\n",
    "contador_2023 = Counter(lista_de_palabras_2023)\n",
    "contador_2024 = Counter(lista_de_palabras_2024)\n",
    "contador_2025 = Counter(lista_de_palabras_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAMOS EL TAMAÑO DESPUES DE INVOCAR LA FUNCION QUE CONTIENE LA DEFINICIÓN DE STOPWORDS\n",
    "print(f\"TAMAÑO 2023:\", len(lista_de_palabras_2023))\n",
    "print(f\"TAMAÑO 2024:\", len(lista_de_palabras_2024))\n",
    "print(f\"TAMAÑO 2025:\", len(lista_de_palabras_2025))\n",
    "print(f\"CONTADOR 2023:\", len(contador_2023))\n",
    "print(f\"CONTADOR 2024:\", len(contador_2024))\n",
    "print(f\"CONTADOR 2025:\", len(contador_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS LA FRECUENCIA DE LAS PRIMERAS N PALABRAS\n",
    "N = 20\n",
    "palabras_mas_comunes_2023 = contador_2023.most_common(N)\n",
    "palabras_mas_comunes_2024 = contador_2024.most_common(N)\n",
    "palabras_mas_comunes_2025 = contador_2025.most_common(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS LAS PALABRAS MÁS REPETIDAS PARA CADA AÑO\n",
    "print(f\"PALABRAS CON MAYOR FRECUENCIA 2023: \\n\", palabras_mas_comunes_2023)\n",
    "print(f\"PALABRAS CON MAYOR FRECUENCIA 2024: \\n\", palabras_mas_comunes_2024)\n",
    "print(f\"PALABRAS CON MAYOR FRECUENCIA 2025: \\n\", palabras_mas_comunes_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LA LIBRERIA QUE UTILIZAREMOS PARA HACER UNA NUBE CONTEO DE PALABRAS\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL WORDCLOUD CON LAS \"PALABRAS MAS COMUNES\"\n",
    "wordcloud_generator = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    # Paleta de colores\n",
    "    colormap='plasma', \n",
    "    # Mostrar máximo 50 palabras\n",
    "    max_words=50,     \n",
    "    # Ya filtramos stop words antes\n",
    "    stopwords=None,  \n",
    "    # Evitar que agrupe palabras (ej. \"dióxido carbono\")\n",
    "    collocations=False  \n",
    ").generate_from_frequencies(contador_2023) # <-- Usar las frecuencias calculadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL GRÁFICO\n",
    "plt.figure(figsize=(10, 5)) # Tamaño de la figura donde se mostrará\n",
    "plt.imshow(wordcloud_generator, interpolation='bilinear') # Mostrar la imagen generada\n",
    "plt.axis(\"off\") # No mostrar los ejes X e Y\n",
    "plt.tight_layout(pad=0) # Ajustar para que no haya bordes extra\n",
    "plt.show() # Mostrar la ventana con la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL WORDCLOUD CON LAS \"PALABRAS MAS COMUNES\"\n",
    "wordcloud_generator = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    # Paleta de colores\n",
    "    colormap='plasma', \n",
    "    # Mostrar máximo 50 palabras\n",
    "    max_words=50,     \n",
    "    # Ya filtramos stop words antes\n",
    "    stopwords=None,  \n",
    "    # Evitar que agrupe palabras (ej. \"dióxido carbono\")\n",
    "    collocations=False  \n",
    ").generate_from_frequencies(contador_2024) # <-- Usar las frecuencias calculadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL GRÁFICO\n",
    "plt.figure(figsize=(10, 5)) # Tamaño de la figura donde se mostrará\n",
    "plt.imshow(wordcloud_generator, interpolation='bilinear') # Mostrar la imagen generada\n",
    "plt.axis(\"off\") # No mostrar los ejes X e Y\n",
    "plt.tight_layout(pad=0) # Ajustar para que no haya bordes extra\n",
    "plt.show() # Mostrar la ventana con la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL WORDCLOUD CON LAS \"PALABRAS MAS COMUNES\"\n",
    "wordcloud_generator = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    # Paleta de colores\n",
    "    colormap='plasma', \n",
    "    # Mostrar máximo 50 palabras\n",
    "    max_words=50,     \n",
    "    # Ya filtramos stop words antes\n",
    "    stopwords=None,  \n",
    "    # Evitar que agrupe palabras (ej. \"dióxido carbono\")\n",
    "    collocations=False  \n",
    ").generate_from_frequencies(contador_2025) # <-- Usar las frecuencias calculadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL GRÁFICO\n",
    "plt.figure(figsize=(10, 5)) # Tamaño de la figura donde se mostrará\n",
    "plt.imshow(wordcloud_generator, interpolation='bilinear') # Mostrar la imagen generada\n",
    "plt.axis(\"off\") # No mostrar los ejes X e Y\n",
    "plt.tight_layout(pad=0) # Ajustar para que no haya bordes extra\n",
    "plt.show() # Mostrar la ventana con la nube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Implementación de \"SPACY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### spaCy es una librería de procesamiento de lenguaje natural (PLN) de código abierto en Python, diseñada para realizar tareas avanzadas de procesamiento del lenguaje de forma eficiente. Es ideal para crear modelos y aplicaciones de producción que requieren análisis de textos, chatbots y otras funciones de análisis de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANCIAMOS LA VARIABLE DEL MODELO (CREAMOS EL MODELO EN \"nlp\")\n",
    "nlp = es_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_palabras = lista_de_palabras_2023 + lista_de_palabras_2024 + lista_de_palabras_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = ' '.join(lista_de_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESAMOS EL TEXTO CON SPACY\n",
    "# nlp: Es el objeto del modelo (es_core_news_lg)\n",
    "# texto_ejemplo: Es un string de texto.\n",
    "# doc: Es el resultado del análisis. Un objeto tipo Doc de spaCy, que contiene tokens, etiquetas gramaticales, entidades nombradas, dependencias sintácticas, etc.\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE TOKENIZACION\n",
    "# RECORREMOS CON UN CICLO FOR EL TEXTO A PROCESAR CREANDO LA TOKENIZACIÓN DEL TEXTO EN CUESTION\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE LEMATIZACIÓN\n",
    "# RECORREMOS LA LISTA ANTERIOR DE TOKENS PARA OBTENER LA LEMATIZACIÓN (FORMA BASE DE CADA TOKEN)\n",
    "for token in doc:\n",
    "    # SI EL VALOR QUE ESTAMOS ANALIZANDO NO ES PUNTUACION Y NO ES ESPACIO GENERAMOS EL LEMMA DE CADA PALABRA\n",
    "    if not token.is_punct and not token.is_space:\n",
    "        print(f\"'{token.text}' -> '{token.lemma_}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE ETIQUETADO GRAMATICAL\n",
    "# token.text: El texto del token\n",
    "# token.pos_: la categoría gramatical general (ej: NOUN, VERB, ADJ...).\n",
    "# spacy.explain(token.pos_): Una descripción en texto de esa categoría.\n",
    "# token.tag_: La etiqueta gramatical más específica (usualmente basada en corpus de entrenamiento).\n",
    "for token in doc:\n",
    "    if not token.is_space: # IGNORAMOS LOS ESPACIOS QUE PUDIERAN PRESENTARSE\n",
    "        print(f\"'{token.text}' -> {token.pos_} ({spacy.explain(token.pos_)}) -> {token.tag_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE ANALISIS DE DEPENDENCIA SINTÁCTICA\n",
    "# token.text: El texto del token (palabra actual)\n",
    "# token.dep_: El tipo de dependencia gramatical (su función sintáctica en la oración: sujeto, objeto, raíz, etc.)\n",
    "# spacy.explain(token.dep_): Una breve explicación textual de esa dependencia\n",
    "# token.head.text: El término principal del que depende ese token (la \"cabeza sintáctica\")\n",
    "for token in doc:\n",
    "     if not token.is_space: # IGNORAMOS LOS ESPACIOS QUE PUDIERAN PRESENTARSE \n",
    "        print(f\"'{token.text}' -> {token.dep_} ({spacy.explain(token.dep_)}) -> '{token.head.text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACION DE ENTIDADES\n",
    "# Las entidades nombradas son cosas como: personas, lugares, organizaciones, fechas, cantidades, etc.\n",
    "# ent.text: el texto de la entidad (ej. \"España\")\n",
    "# ent.label_: la etiqueta de tipo de entidad (ej. LOC)\n",
    "# spacy.explain(ent.label_): una descripción en texto del tipo (ej. \"Geopolitical entity\")\n",
    "if doc.ents:\n",
    "    print(\"Entidades encontradas:\")\n",
    "    print(\"Texto de la Entidad -> Etiqueta (Tipo)\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"'{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "else:\n",
    "    print(\"No se encontraron entidades nombradas en este texto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACION DE ENTIDADES\n",
    "displacy.render(doc,style='ent',jupyter=True,options={'distance':200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTRAMOS LAS ENTIDADES LOC Y ORG\n",
    "entidades_loc = [ent.text for ent in doc.ents if ent.label_ in ['LOC']]\n",
    "entidades_org = [ent.text for ent in doc.ents if ent.label_ in ['ORG']]\n",
    "entidades_per = [ent.text for ent in doc.ents if ent.label_ in ['PER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir todo como un solo texto para WordCloud\n",
    "texto_entidades_loc = ' '.join(entidades_loc)\n",
    "texto_entidades_org = ' '.join(entidades_org)\n",
    "texto_entidades_per = ' '.join(entidades_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[1mENTIDADES TIPO LOC:\\033[0m \\n\", texto_entidades_loc)\n",
    "print(f\"\\033[1mENTIDADES TIPO ORG:\\033[0m \\n\", texto_entidades_org)\n",
    "print(f\"\\033[1mENTIDADES TIPO PER:\\033[0m \\n\", texto_entidades_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar entidades compuestas por una sola palabra con guiones bajos\n",
    "texto_entidades_loc = texto_entidades_loc.replace(\"san nicolás\", \"san_nicolás\")\n",
    "texto_entidades_loc = texto_entidades_loc.replace(\"ciudad rosario\", \"ciudad_rosario\")\n",
    "texto_entidades_loc = texto_entidades_loc.replace(\"carlos paz\", \"carlos_paz\")\n",
    "texto_entidades_loc = texto_entidades_loc.replace(\"mina clavero\", \"mina_clavero\")\n",
    "texto_entidades_loc = texto_entidades_loc.replace(\"buenos aires\", \"buenos_aires\")\n",
    "\n",
    "texto_entidades_org = texto_entidades_org.replace(\"federación rosarina\", \"federación_rosarina\")\n",
    "texto_entidades_org = texto_entidades_org.replace(\"yiya team\", \"yiya_team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[1mENTIDADES TIPO LOC:\\033[0m \\n\", texto_entidades_loc)\n",
    "print(f\"\\033[1mENTIDADES TIPO ORG:\\033[0m \\n\", texto_entidades_org)\n",
    "print(f\"\\033[1mENTIDADES TIPO PER:\\033[0m \\n\", texto_entidades_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS Y MOSTRAMOS EL WORDCLOUD PARA LOC\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_entidades_loc)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS Y MOSTRAMOS EL WORDCLOUD PARA ORG\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_entidades_org)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN2LqXDAoFNo",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.- Implementación de “BAG OF WORDS” (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La bag of words (BoW; también llamada bolsa de palabras) es una técnica de extracción de características que modela datos de texto para su procesamiento en algoritmos de recuperación de información y machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "from sklearn.feature_extraction.text import CountVectorizer # CREAMOS LA MATRIZ DE PALABRAS\n",
    "import nltk\n",
    "# CountVectorizer: Convierte un conjunto de textos en vectores numéricos, donde cada columna representa una palabra del vocabulario y cada fila \n",
    "# representa un texto. Es lo que se llama una bolsa de palabras (bag of words): solo cuenta cuántas veces aparece cada palabra en el texto \n",
    "# (sin importar el orden).\n",
    "\n",
    "# NLTK es una libreria que ofrece herramientas para:\n",
    "# * Tokenización (dividir texto en palabras, frases, oraciones)\n",
    "# * Lematización y stemming\n",
    "# * Etiquetado gramatical (POS tagging)\n",
    "# * Extracción de entidades\n",
    "# * Corpus integrados (textos de ejemplo)\n",
    "# * Modelos de lenguaje sencillos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords') # DESCARGAMOS EL MODULO \"stopwords\"\n",
    "sw_espaniol = nltk.corpus.stopwords.words('spanish') # ASIGNAMOS LAS \"stopwords\" DEL ESPAÑOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS LAS STOPWORDS DEL ESPAÑOL\n",
    "print(sw_espaniol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3B7cFlxoQYA"
   },
   "outputs": [],
   "source": [
    "# INSTANCIAMOS UNA VARIABLE DE TIPO \"CountVectorizer\" Y LE INDICAMOS QUE CUENTA AHORA CON UN \"stop_words\" DEFINIDO\n",
    "count_vect = CountVectorizer(stop_words=sw_espaniol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORTAMOS LIBRERIAS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_a_procesar = texto.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(texto_a_procesar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_a_procesar_df = pd.DataFrame(texto_a_procesar, columns=['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_a_procesar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAMOS EL TEXTO EN DATOS NUMÉRICOS USANDO \"CountVectorizer\" PARA CONVERTIR LOS TEXTOS EN UNA MATRIZ.\n",
    "data_cv = count_vect.fit_transform(texto_a_procesar_df.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAMOS UN DATAFRAME TRANSFORMANDO CON LA MATRIZ QUE GENERAMOS ANTES DONDE:\n",
    "# 1.- LAS COLUMNAS SON LAS PALABRAS\n",
    "# 2.- LAS FILAS SON LOS DOCUMENTOS\n",
    "# DTM: DATA EN FORMATO DATAFRAME\n",
    "data_dtf = pd.DataFrame(data_cv.toarray(), columns=count_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_dtf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS LA MATRIZ EN FORMATO DATAFRAME\n",
    "data_dtf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvBAJHZLwYHX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.- Análisis Exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.- CARGAMOS EL DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FIA5B2z5pUFP",
    "outputId": "690be42b-beb0-40c2-aaab-7cc9c5cca63a"
   },
   "outputs": [],
   "source": [
    "data_dtf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSPONEMOS LA MATRIZ PARA TENER UNA EN FORMATO \"terminosXdocumentos\"\n",
    "data_dtf = data_dtf.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAMOS LOS PRIMEROS 10 REGISTROS\n",
    "# AHORA CONTAMOS CON LAS PALABRAS DEL TEXTO EN LAS FILAS Y LAS OCURRENCIAS EN LA COLUMNA\n",
    "data_dtf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.- BUSCAMOS LAS PALABRAS MAS USADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMAMOS LA CANTIDAD DE 1 DE CADA FILA\n",
    "suma_filas = data_dtf.sum(axis=1)\n",
    "# CREAMOS UN NUEVO DATAFRAME CON DOS COLUMNAS \"PALABRA\" Y \"TOTAL\"\n",
    "nuevo_data_dtf = pd.DataFrame({\n",
    "    'palabra': suma_filas.index,\n",
    "    'total': suma_filas.values\n",
    "})\n",
    "# BUSCAMOS VER SOLO LAS PRIMERAS 50 FILAS (CON EL MAYOR NUMERO DE PALABRAS)\n",
    "top_50_df = nuevo_data_dtf.sort_values(by='total', ascending=False).head(50)\n",
    "# VISUALIZAMOS EL DATAFRAME CREADO\n",
    "print(top_50_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_50_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERIAS\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el objeto WordCloud con determinados parametros y utilizando nuestra lista de stopwords\n",
    "wc = WordCloud(stopwords=sw_espaniol, background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que tenés 'palabra' y 'total' como columnas\n",
    "# GENERAMOS UN DICCIONARIO CON LA FRECUENCIA DE CADA PALABRA\n",
    "# KEY: PALABRA\n",
    "# VALUE: TOTAL\n",
    "frecuencias = dict(zip(top_50_df['palabra'], top_50_df['total']))\n",
    "\n",
    "# DEFINIMOS EL FORMATO DEL WORDPLOUD\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frecuencias)\n",
    "\n",
    "# GENERAMOS EL GRÁFICO A VISUALIZAR\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
