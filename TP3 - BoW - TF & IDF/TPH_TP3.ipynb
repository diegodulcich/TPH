{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 - Baf Of Words - Term Frequency & Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJEMPLO 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.- IMPORTAMOS LAS LIBRERIAS Y DEFINIMOS EL TEXTO A ANALIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALAMOS LA LIBRERIA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAMOS LA LIBRERIA PARA PODER INVOCARLA\n",
    "documentos = [\n",
    "    \"El perro ladró fuerte cuando pasó la ambulancia.\",\n",
    "    \"María compró frutas frescas en el mercado central.\",\n",
    "    \"Los niños jugaban felices bajo el sol de verano.\",\n",
    "    \"A veces la lluvia trae recuerdos de viejos tiempos.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4k2pDwBtjWy"
   },
   "outputs": [],
   "source": [
    "print(type(documentos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nuestro CORPUS de ejemplo:\")\n",
    "for i, doc in enumerate(documentos):\n",
    "    print(f\"Oración {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.- PROCESAMOS EL TEXTO A ANALIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS EL \"BAG OF WORDS\"\n",
    "# INSTANCIAMOS UNA VARIABLE DE TIPO \"CountVectorizer\"\n",
    "# CountVectorizer: Convierte un conjunto de textos en vectores numéricos, donde cada columna representa una palabra del vocabulario y cada fila \n",
    "# representa un texto. Es lo que se llama una bolsa de palabras (bag of words): solo cuenta cuántas veces aparece cada palabra en el texto \n",
    "# (sin importar el orden).\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW ES NUESTRA BAG OF WORDS // DICCIONARIO DE PALABRAS\n",
    "bow = count_vect.fit_transform(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAMOS CUANTAS VECES APARECE CADA PALABRA EN EL TEXTO\n",
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())\n",
    "print(bow[3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAMOS UN DATAFRAME CON LAS PALABRAS DE NUESTRO VOCABULARIO (COLUMNAS) Y LAS FILAS NUESTRAS ORACIONES\n",
    "df_bow = pd.DataFrame(bow.toarray(), columns=vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAMOS NUESTRA MATRIZ DE \"FEATURE EXTRACTION\"\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.- RESUMEN DE LA TÉCNICA \"FEATURE EXTRACTION\" (EXTRACCIÓN DE CARACTERÍSTICAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUMEN DE LA TECNICA\n",
    "\n",
    "# PROS\n",
    "# - SIMPLE\n",
    "# - FACIL\n",
    "# - EXPLICABLE\n",
    "\n",
    "# CONTRAS\n",
    "# - PALABRAS COMPUESTAS (N-GRAMS)\n",
    "# - POLISEMIA (DOS PALABRAS QUE SIGNIFICAN DISTINTAS COSAS EN DETERMINADO CONTEXTO)\n",
    "# - CORRELACION DE PALABRAS\n",
    "# - ORDEN\n",
    "# - ESCASEZ (UNA PALABRA ES IMPORTANTE PERO APARECE MUY POCAS VECES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xo4mA7A4uUXN"
   },
   "outputs": [],
   "source": [
    "# DEFINIMOS UN TEXTO A PROCESAR \n",
    "texto_ejemplo = 'La inteligencia artificial es un campo de la ciencia relacionado con la creación de computadoras y máquinas que pueden razonar, aprender y actuar de una manera que normalmente requeriría inteligencia humana o que involucra datos cuya escala excede lo que los humanos pueden analizar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE DE SPACY\n",
    "# CARGAMOS EL MODELO\n",
    "import es_core_news_lg\n",
    "nlp = es_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptTnyA9FuyCF"
   },
   "outputs": [],
   "source": [
    "# PROCESAMOS EL TEXTO CON SPACY\n",
    "# nlp: Es el objeto del modelo (es_core_news_lg)\n",
    "# texto_ejemplo: Es un string de texto.\n",
    "# doc: Es el resultado del análisis. Un objeto tipo Doc de spaCy, que contiene tokens, etiquetas gramaticales, entidades nombradas, dependencias sintácticas, etc.\n",
    "doc = nlp(texto_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIRMAMOS QUE \"doc\" ES UN OBJETO SPACY\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS EL CONTENIDO\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKPu-lHBvsfQ",
    "outputId": "da8d2518-208e-449f-e54a-53509401570a"
   },
   "outputs": [],
   "source": [
    "# PROCESO DE TOKENIZACION\n",
    "# RECORREMOS CON UN CICLO FOR EL TEXTO A PROCESAR CREANDO LA TOKENIZACIÓN DEL TEXTO EN CUESTION\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS QUE OBTUVIMOS UNA LISTA CON LOS TOKENS DEL TEXTO EN CUESTION\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-3fvF7Lwl5x",
    "outputId": "640305cd-73db-45d2-bced-434ac5018ce5"
   },
   "outputs": [],
   "source": [
    "# PROCESO DE LEMATIZACIÓN\n",
    "# RECORREMOS LA LISTA ANTERIOR DE TOKENS PARA OBTENER LA LEMATIZACIÓN (FORMA BASE DE CADA TOKEN)\n",
    "for token in doc:\n",
    "    # SI EL VALOR QUE ESTAMOS ANALIZANDO NO ES PUNTUACION Y NO ES ESPACIO GENERAMOS EL LEMMA DE CADA PALABRA\n",
    "    if not token.is_punct and not token.is_space:\n",
    "        print(f\"'{token.text}' -> '{token.lemma_}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lxtMfBXxgMl",
    "outputId": "53919f54-2fb2-4e3c-9794-6710410a5f11"
   },
   "outputs": [],
   "source": [
    "# PROCESO DE ETIQUETADO GRAMATICAL\n",
    "# token.text: El texto del token\n",
    "# token.pos_: la categoría gramatical general (ej: NOUN, VERB, ADJ...).\n",
    "# spacy.explain(token.pos_): Una descripción en texto de esa categoría.\n",
    "# token.tag_: La etiqueta gramatical más específica (usualmente basada en corpus de entrenamiento).\n",
    "for token in doc:\n",
    "    if not token.is_space: # IGNORAMOS LOS ESPACIOS QUE PUDIERAN PRESENTARSE\n",
    "        print(f\"'{token.text}' -> {token.pos_} ({spacy.explain(token.pos_)}) -> {token.tag_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC2baEuzyduK",
    "outputId": "1fd4dc34-573f-4546-dbb1-8a9f71d3fd2f"
   },
   "outputs": [],
   "source": [
    "# PROCESO DE ANALISIS DE DEPENDENCIA SINTÁCTICA\n",
    "# token.text: El texto del token (palabra actual)\n",
    "# token.dep_: El tipo de dependencia gramatical (su función sintáctica en la oración: sujeto, objeto, raíz, etc.)\n",
    "# spacy.explain(token.dep_): Una breve explicación textual de esa dependencia\n",
    "# token.head.text: El término principal del que depende ese token (la \"cabeza sintáctica\")\n",
    "for token in doc:\n",
    "     if not token.is_space: # IGNORAMOS LOS ESPACIOS QUE PUDIERAN PRESENTARSE \n",
    "        print(f\"'{token.text}' -> {token.dep_} ({spacy.explain(token.dep_)}) -> '{token.head.text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "khIG9P4Oy06W",
    "outputId": "5ae60226-ca6a-4532-c27e-749e18e8a9ad"
   },
   "outputs": [],
   "source": [
    "# PROCESO DE VISUALIZACION DEL OBEJTO PROCESADO POR SPACY\n",
    "# displacy.render: función que dibuja una visualización del objeto doc (procesado por spaCy).\n",
    "# style='dep': indica que quieres ver el árbol de dependencias gramaticales (no entidades nombradas).\n",
    "# jupyter=True: le dice a spaCy que lo dibuje directamente dentro del notebook de Jupyter.\n",
    "# options={'distance': 120}: cambia la distancia horizontal entre palabras en el gráfico (para que sea más legible si la oración es larga).\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdCC4UHXzl5O",
    "outputId": "f0057dbf-55ce-4047-a162-6932bcf8d7f4"
   },
   "outputs": [],
   "source": [
    "# VISUALIZACION DE ENTIDADES\n",
    "# Las entidades nombradas son cosas como: personas, lugares, organizaciones, fechas, cantidades, etc.\n",
    "# ent.text: el texto de la entidad (ej. \"España\")\n",
    "# ent.label_: la etiqueta de tipo de entidad (ej. LOC)\n",
    "# spacy.explain(ent.label_): una descripción en texto del tipo (ej. \"Geopolitical entity\")\n",
    "if doc.ents:\n",
    "    print(\"Entidades encontradas:\")\n",
    "    print(\"Texto de la Entidad -> Etiqueta (Tipo)\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"'{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "else:\n",
    "    print(\"No se encontraron entidades nombradas en este texto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "YiENpGV8z_lA",
    "outputId": "51b13385-7086-41eb-d057-f5d541f2fe16"
   },
   "outputs": [],
   "source": [
    "# VISUALIZACION DE ENTIDADES\n",
    "displacy.render(doc,style='ent',jupyter=True,options={'distance':200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS UN TEXTO A PROCESAR \n",
    "texto_ejemplo = '¿Quién liderará la tecnología del futuro: Estados Unidos o China? La especulación está en su punto máximo. La compañía de inteligencia artificial más famosa del mundo, OpenAI, es estadounidense. Los modelos producidos por DeepSeek, un competidor chino, son casi igual de buenos y más baratos.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESAMOS EL TEXTO CON SPACY\n",
    "# nlp: Es el objeto del modelo (es_core_news_lg)\n",
    "# texto_ejemplo: Es un string de texto.\n",
    "# doc: Es el resultado del análisis. Un objeto tipo Doc de spaCy, que contiene tokens, etiquetas gramaticales, entidades nombradas, dependencias sintácticas, etc.\n",
    "doc = nlp(texto_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIRMAMOS QUE \"doc\" ES UN OBJETO SPACY\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAMOS EL CONTENIDO\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE TOKENIZACION\n",
    "# RECORREMOS CON UN CICLO FOR EL TEXTO A PROCESAR CREANDO LA TOKENIZACIÓN DEL TEXTO EN CUESTION\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAMOS QUE OBTUVIMOS UNA LISTA CON LOS TOKENS DEL TEXTO EN CUESTION\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE LEMATIZACIÓN\n",
    "# RECORREMOS LA LISTA ANTERIOR DE TOKENS PARA OBTENER LA LEMATIZACIÓN (FORMA BASE DE CADA TOKEN)\n",
    "for token in doc:\n",
    "    # SI EL VALOR QUE ESTAMOS ANALIZANDO NO ES PUNTUACION Y NO ES ESPACIO GENERAMOS EL LEMMA DE CADA PALABRA\n",
    "    if not token.is_punct and not token.is_space:\n",
    "        print(f\"'{token.text}' -> '{token.lemma_}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE ETIQUETADO GRAMATICAL\n",
    "# token.text: El texto del token\n",
    "# token.pos_: la categoría gramatical general (ej: NOUN, VERB, ADJ...).\n",
    "# spacy.explain(token.pos_): Una descripción en texto de esa categoría.\n",
    "# token.tag_: La etiqueta gramatical más específica (usualmente basada en corpus de entrenamiento).\n",
    "for token in doc:\n",
    "    if not token.is_space: # IGNORAMOS LOS ESPACIOS QUE PUDIERAN PRESENTARSE\n",
    "        print(f\"'{token.text}' -> {token.pos_} ({spacy.explain(token.pos_)}) -> {token.tag_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE ANALISIS DE DEPENDENCIA SINTÁCTICA\n",
    "# token.text: El texto del token (palabra actual)\n",
    "# token.dep_: El tipo de dependencia gramatical (su función sintáctica en la oración: sujeto, objeto, raíz, etc.)\n",
    "# spacy.explain(token.dep_): Una breve explicación textual de esa dependencia\n",
    "# token.head.text: El término principal del que depende ese token (la \"cabeza sintáctica\")\n",
    "for token in doc:\n",
    "     if not token.is_space: # IGNORAMOS LOS ESPACIOS QUE PUDIERAN PRESENTARSE \n",
    "        print(f\"'{token.text}' -> {token.dep_} ({spacy.explain(token.dep_)}) -> '{token.head.text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESO DE VISUALIZACION DEL OBEJTO PROCESADO POR SPACY\n",
    "# displacy.render: función que dibuja una visualización del objeto doc (procesado por spaCy).\n",
    "# style='dep': indica que quieres ver el árbol de dependencias gramaticales (no entidades nombradas).\n",
    "# jupyter=True: le dice a spaCy que lo dibuje directamente dentro del notebook de Jupyter.\n",
    "# options={'distance': 120}: cambia la distancia horizontal entre palabras en el gráfico (para que sea más legible si la oración es larga).\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACION DE ENTIDADES\n",
    "# Las entidades nombradas son cosas como: personas, lugares, organizaciones, fechas, cantidades, etc.\n",
    "# ent.text: el texto de la entidad (ej. \"España\")\n",
    "# ent.label_: la etiqueta de tipo de entidad (ej. LOC)\n",
    "# spacy.explain(ent.label_): una descripción en texto del tipo (ej. \"Geopolitical entity\")\n",
    "if doc.ents:\n",
    "    print(\"Entidades encontradas:\")\n",
    "    print(\"Texto de la Entidad -> Etiqueta (Tipo)\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"'{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "else:\n",
    "    print(\"No se encontraron entidades nombradas en este texto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACION DE ENTIDADES\n",
    "displacy.render(doc,style='ent',jupyter=True,options={'distance':200})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
